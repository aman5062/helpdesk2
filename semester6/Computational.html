<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta meta name="viewport" content="width=device-width, user-scalable=no" />
    <title>Helpdesk</title>
</head>
   
<script
	src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js"
	integrity="sha384-7+zCNj/IqJ95wo16oMtfsKbZ9ccEh31eOz1HGyDuCQ6wgnyJNSYdrPa03rtR1zdB"
	crossorigin="anonymous"
></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link
	rel="stylesheet"
	href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.7.2/font/bootstrap-icons.css"
/>
<link rel="icon" href="assets/images/logo.png" type="image/png">
<link rel="stylesheet" type="text/css" href="../assets/css/style.css" />
<script src="../assets/js/script.js"></script>
<style>
 
  </style>
<body  onload="waterm()">
  <header class="navbar navbar-expand-md d-flex flex-wrap justify-content-center mt-2 p-2 mb-2 border-bottom">
   <div id="top_bar">
    <a href="#" class="d-flex align-items-center mb-3 mb-md-0 me-md-auto link-body-emphasis text-decoration-none">
      <img src="../assets/images/logo.png" id="imglogo">
      <span class="fs-4 mr-5 lo"><h2 class="bold-text">Helpdesk</h2></span>
    </a>
   </div>
    <div class="container-fluid" id="sec_nav">
      <button class="navbar-toggler" type="button" onclick="openLeftSidebar()">
      <span class="navbar-toggler-icon"></span>
      </button>
     <div id="leftSidebar" class="sidebar-left">
      <a href="javascript:void(0)" class="closebtn" onclick="closeLeftSidebar()">&times;</a>
      <h2>semester 5</h2>
      <a href="../semester5/index.html">Software tech.</a>
      <a href="../semester5/formal.html">Formal lang.</a>
      <a href="../semester5/ai.html">AI</a>
      <a href="../semester5/ml.html">ML</a>
      <a href="../semester5/const.html">Indian Const.</a>
      <a href="../semester5/comp_netw.html">Comp. Networks</a>
      <a href="../semester5/big.html">Big Data</a>
      <a href="../semester5/cloud.html">Cloud Computing</a>

      <h2>semester 6</h2>
      <a href="../semester6/index.html">Digital Image</a>
      <a href="../semester6/Compiler.html">Compiler Design</a>
      <a href="../semester6/Computational.html" class="active">Computational Intell.</a>
      <a href="../semester6/ds_big_data.html">DS and Big data</a>
      <a href="../semester6/computer_graphics.html">Computer Graphics</a>
      <a href="../semester6/python_da.html">Python for DA</a>
      <a href="../semester6/ds.html">Data Science</a>
     
  </div>
  <div class="semester">semester-6</div>

      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" id="upper_menu_btn" data-bs-target="#collapsibleNavbar">
 <i class="fa fa-arrow-down" aria-hidden="true"></i>      </button>
      <div class="collapse navbar-collapse" id="collapsibleNavbar" >
        <ul class="nav nav-pills navbar-nav ms-auto" id="myNav">
          <!-- 1st Year -->
          <li class="nav-item">
            <a class="nav-link" href="../semester1/index.html">1st Year</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../semester3/index.html">2nd Year</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../semester5/index.html">3rd Year</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../semester7/index.html">4th Year</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../NPTEL/index.html">NPTEL</a>
          </li>
        </ul>
      </div>
    </div>
  </header>
  
  
    <div id="body">
      <div class="heading_name">Computational Intelligence</div><br>
      
<mark><strong>Module :- 01</strong></mark><br><br>
      <mark><que>1. What is Soft Computing?</que></mark>
      <ans>
          <p>Soft computing is a branch of computational intelligence that mimics human decision-making capabilities and reasoning. Unlike hard computing, which relies on precise and exact solutions, soft computing uses approximate reasoning and uncertainty handling to find flexible and adaptable solutions for complex real-world problems.</p>
          <ul>
              <li><b>Key Characteristics:</b></li>
              <ul>
                  <li>Tolerance for imprecision.</li>
                  <li>Ability to handle non-linearity.</li>
                  <li>Works well in scenarios where no exact solution exists.</li>
              </ul>
              <li><b>Examples:</b> Pattern recognition (e.g., facial recognition), predictive modeling (e.g., weather forecasting).</li>
          </ul>
          <p>Soft computing relies on interdisciplinary approaches, integrating methodologies like fuzzy logic, neural networks, and genetic algorithms to simulate human intelligence. These methods empower machines to work in uncertain and imprecise environments effectively.</p>
      </ans>

      <mark><que>2. What are the Differences Between Soft and Hard Computing?</que></mark>
    <ans>
        <p>The following table outlines the major differences between soft computing and hard computing:</p>
        <table border="1">
            <tr>
                <th>Feature</th>
                <th>Soft Computing</th>
                <th>Hard Computing</th>
            </tr>
            <tr>
                <td>Precision</td>
                <td>Relies on approximate and probabilistic solutions.</td>
                <td>Requires exact and deterministic solutions.</td>
            </tr>
            <tr>
                <td>Nature</td>
                <td>Flexible, adaptable, and human-like reasoning.</td>
                <td>Rigid, rule-based, and algorithmic.</td>
            </tr>
            <tr>
                <td>Error Tolerance</td>
                <td>High tolerance for errors and noise.</td>
                <td>Low tolerance for errors.</td>
            </tr>
            <tr>
                <td>Application Areas</td>
                <td>Complex, real-world problems like image processing.</td>
                <td>Mathematical problems like arithmetic.</td>
            </tr>
            <tr>
                <td>Examples</td>
                <td>Neural networks, fuzzy logic, genetic algorithms.</td>
                <td>Traditional algorithms, numeric computing.</td>
            </tr>
        </table>
        <p>While soft computing thrives in scenarios requiring approximate solutions, hard computing is indispensable for tasks demanding absolute precision, such as banking systems or scientific calculations.</p>
    </ans>

    <mark><que>3. What are the Components of Soft Computing?</que></mark>
    <ans>
        <p>Soft computing integrates various methodologies, including Artificial Intelligence (AI), Neural Networks, Fuzzy Logic, and Genetic Algorithms. Each component is described below:</p>

        <!-- Artificial Intelligence -->
        <mark><que>3.1 What is Artificial Intelligence?</que></mark>
        <ans>
            <p>Artificial Intelligence (AI) refers to the simulation of human intelligence by machines. It enables computers to perform tasks that typically require human cognition, such as learning, reasoning, and problem-solving.</p>
            <ul>
                <li><b>Key Features:</b></li>
                <ul>
                    <li><b>Learning:</b> AI systems improve performance over time through training data.</li>
                    <li><b>Reasoning:</b> Capable of making decisions based on logical inference.</li>
                    <li><b>Adaptability:</b> Handles dynamic and unpredictable environments.</li>
                </ul>
                <li><b>Applications:</b> Autonomous vehicles, natural language processing, robotics, and automation.</li>
            </ul>
            <p>Modern AI integrates technologies such as deep learning and reinforcement learning to address more intricate challenges like gaming strategy or drug discovery.</p>
        </ans>
        <mark><que>3.2 What are Neural Networks?</que></mark>
        <ans>
            <p>A Neural Network is a computational model inspired by the structure of the human brain. It is composed of interconnected nodes (neurons) that process information in a layered architecture.</p>
            <ul>
                <li><b>Key Features:</b></li>
                <ul>
                    <li><b>Learning Process:</b> Neural networks use a technique called backpropagation to adjust weights and minimize error.</li>
                    <li><b>Structure:</b> Includes input layers, hidden layers, and output layers.</li>
                    <li><b>Adaptability:</b> Excels in tasks like image classification, speech recognition, and prediction.</li>
                </ul>
                <li><b>Applications:</b> Fraud detection, language translation, and autonomous systems.</li>
            </ul>
            <p>Advancements in neural networks, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), have significantly boosted capabilities in image processing and sequential data handling, respectively.</p>
        </ans>

        <mark><que>3.3 What is Fuzzy Logic?</que></mark>
        <ans>
            <p>Fuzzy Logic is a mathematical framework for dealing with uncertainty and imprecision. Instead of binary true/false values, it uses degrees of truth, enabling more human-like reasoning.</p>
            <ul>
                <li><b>Key Features:</b></li>
                <ul>
                    <li>Fuzzy sets represent how much a value belongs to a set.</li>
                    <li>Rule-based systems rely on "if-then" rules.</li>
                    <li>Highly flexible and adaptable to vague information.</li>
                </ul>
                <li><b>Applications:</b> Washing machines, climate control systems, and decision-making systems.</li>
            </ul>
            <p>Fuzzy logic enhances decision-making in environments where binary decision-making fails to provide adequate flexibility.</p>
        </ans>

        <mark><que>3.4 What are Genetic Algorithms?</que></mark>
        <ans>
            <p>Genetic Algorithms are optimization techniques based on the principles of natural selection and evolution. They are used to find the best solution in large and complex search spaces.</p>
            <ul>
                <li><b>Key Features:</b></li>
                <ul>
                    <li>Encoded solutions are represented as chromosomes.</li>
                    <li>A fitness function measures the quality of each solution.</li>
                    <li>Genetic operators like selection, crossover, and mutation evolve better solutions.</li>
                </ul>
                <li><b>Steps:</b>
                    <ol>
                        <li>Generate an initial population.</li>
                        <li>Evaluate fitness.</li>
                        <li>Perform crossover and mutation.</li>
                        <li>Repeat until an optimal solution is found.</li>
                    </ol>
                </li>
                <li><b>Applications:</b> Scheduling problems, route optimization, and feature selection in machine learning.</li>
            </ul>
            <p>Through iterative evolution, genetic algorithms efficiently address problems with vast solution spaces that traditional methods struggle to optimize.</p>
        </ans>
    </ans>
<br>

<mark><strong>Module :- 02</strong></mark><br><br>
<mark><que>1. What is Fuzzy Set Theory?</que></mark>
<ans>
    <p>Fuzzy set theory is an extension of classical set theory that allows for partial membership in a set. Unlike crisp sets, where an element is either a member or not (binary logic: 0 or 1), fuzzy sets define membership using a degree that ranges between 0 and 1. This approach enables us to represent and reason with uncertainty and vagueness present in real-world scenarios.</p>
    <p>Fuzzy set theory is widely used in areas such as control systems, decision-making, and artificial intelligence, where binary categorizations are insufficient. In essence, fuzzy sets generalize the concept of a "set" to handle imprecision and subjectivity effectively.</p>
    <p><b>Example:</b> A fuzzy set "Tall People" might assign a membership degree of 0.8 to a person who is 6 feet tall, whereas a crisp set would categorize the person as either "tall" or "not tall."</p>
    <b>Diagram:</b>
    <pre>
Membership Degree
1.0 |                    ________
|                   /
0.8 |          _______/
|         /
0.6 |        /
|       /
0.4 |      /
|     /
0.2 |    /
|___/________________________
   160  165  170  175  180  (Height in cm)
    </pre>
</ans>

<mark><que>2. Fuzzy Set vs Crisp Set</que></mark>
<ans>
    <p>The difference between a fuzzy set and a crisp set lies in how they handle membership. In a crisp set, an element belongs either fully or not at all, while in a fuzzy set, an element can have partial membership characterized by a degree ranging between 0 and 1.</p>
    <b>Comparison Table:</b>
    <table border="1">
        <tr>
            <th>Feature</th>
            <th>Fuzzy Set</th>
            <th>Crisp Set</th>
        </tr>
        <tr>
            <td>Membership</td>
            <td>Partial membership with degrees (0 to 1)</td>
            <td>Full membership (0 or 1)</td>
        </tr>
        <tr>
            <td>Uncertainty</td>
            <td>Handles uncertainty and vagueness</td>
            <td>Does not handle uncertainty</td>
        </tr>
        <tr>
            <td>Example</td>
            <td>"Tall people" where height = 6ft may have a membership of 0.8</td>
            <td>"Tall people" where height = 6ft is either in or out of the set</td>
        </tr>
    </table>
</ans>

<mark><que>3. Crisp Relation vs Fuzzy Relation</que></mark>
<ans>
    <p>A crisp relation is a subset of the Cartesian product of two sets, where each pair is either included or not (binary logic). On the other hand, a fuzzy relation extends this concept by associating each pair with a membership degree, representing the strength or intensity of the relationship.</p>
    <b>Comparison Table:</b>
    <table border="1">
        <tr>
            <th>Feature</th>
            <th>Crisp Relation</th>
            <th>Fuzzy Relation</th>
        </tr>
        <tr>
            <td>Membership</td>
            <td>Binary (0 or 1)</td>
            <td>Range from 0 to 1</td>
        </tr>
        <tr>
            <td>Representation</td>
            <td>Matrix with entries 0 or 1</td>
            <td>Matrix with entries in [0, 1]</td>
        </tr>
        <tr>
            <td>Example</td>
            <td>Relation between cities with direct flights (1 if direct, 0 if not)</td>
            <td>Relation between cities with membership representing flight frequency</td>
        </tr>
    </table>
</ans>

<mark><que>4. What are Membership Functions?</que></mark>
<ans>
    <p>Membership functions define how each element in the universe of discourse is mapped to a membership degree in a fuzzy set. They are a key component of fuzzy logic, enabling the representation of vagueness and uncertainty in mathematical terms. A membership function is typically represented as a curve that maps each input value to a degree between 0 and 1.</p>
    <b>Types of Membership Functions:</b>
    <ul>
        <li><b>Triangular:</b> Defined by three points (a, b, c) where the peak represents full membership.</li>
        <li><b>Trapezoidal:</b> Similar to triangular but with a flat top, defined by four points (a, b, c, d).</li>
        <li><b>Gaussian:</b> Smooth and symmetric, defined by a mean and standard deviation.</li>
    </ul>
    <p><b>Applications:</b> Membership functions are used in fuzzy control systems, where they help in decision-making by providing a mathematical representation of linguistic terms like "high," "medium," or "low."</p>
    <b>Diagram:</b>
    <pre>
Membership Degree
1.0 |       /\\
|      /  \\
0.5 |     /    \\
|____/______\\______
 a    b      c
    </pre>
</ans>

<mark><que>5. Fuzzy Rule-Based System</que></mark>
<ans>
    <p>A fuzzy rule-based system is a decision-making framework that uses fuzzy logic to model complex systems. It relies on a set of "if-then" rules to map inputs to outputs based on linguistic terms. These rules use fuzzy sets and membership functions to handle uncertainty and vagueness.</p>
    <b>Components:</b>
    <ul>
        <li><b>Fuzzy Rules:</b> Statements that define the relationship between inputs and outputs.</li>
        <li><b>Fuzzy Inference Engine:</b> Processes the rules and combines them to derive a fuzzy output.</li>
        <li><b>Defuzzification:</b> Converts the fuzzy output into a crisp value.</li>
    </ul>
    <b>Example:</b>
    <pre>
IF temperature is "high" AND humidity is "low" THEN fan_speed is "medium."
    </pre>
    <p><b>Applications:</b> Used in systems like climate control, washing machines, and autonomous vehicles.</p>
</ans>

<mark><que>6. Fuzzy Inference Systems</que></mark>
<ans>
    <p>Fuzzy inference systems (FIS) are frameworks for reasoning and decision-making based on fuzzy logic. They process inputs using fuzzy rules and membership functions to generate a fuzzy output, which is then defuzzified to produce a crisp result.</p>
    <b>Types of FIS:</b>
    <ul>
        <li><b>Mamdani FIS:</b> Uses fuzzy sets for both input and output.</li>
        <li><b>Sugeno FIS:</b> Uses fuzzy sets for input and mathematical functions for output.</li>
    </ul>
    <p><b>Example:</b> In an air conditioning system, FIS can adjust the cooling based on fuzzy inputs like "hot" and "humid."</p>
    <b>Diagram:</b>
    <pre>
Input -> Fuzzification -> Rule Evaluation -> Aggregation -> Defuzzification -> Output
    </pre>
</ans>


<!-- module-3  -->
<mark><strong>Module :- 03</strong></mark><br><br>

<h1><mark>Artificial Neural Networks vs Biological Neural Networks</mark></h1>

<h2><mark>Biological Neural Network (BNN)</mark></h2>
<h3>Neuron Anatomy:</h3>
<ul>
  <li><strong>Dendrites:</strong> Receive input signals.</li>
  <li><strong>Soma (Cell Body):</strong> Processes input.</li>
  <li><strong>Axon:</strong> Sends output to other neurons.</li>
</ul>
<p><strong>Signal Propagation:</strong> Through synapses via chemical signals.</p>
<p><strong>Learning:</strong> Adjusts synaptic strength based on experience.</p>

<h2><mark>Artificial Neural Network (ANN)</mark></h2>
<ul>
  <li><strong>Artificial Neuron:</strong> Modeled to imitate a biological neuron.</li>
  <li><strong>Weights:</strong> Represent the strength of connections.</li>
  <li><strong>Learning:</strong> Uses algorithms like backpropagation to adjust weights.</li>
</ul>
<h2><mark>Comparison Table: BNN vs ANN</mark></h2>
<table border="1" cellpadding="8" cellspacing="0">
  <thead>
    <tr>
      <th>Feature</th>
      <th>BNN</th>
      <th>ANN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Basic Unit</td>
      <td>Biological neuron</td>
      <td>Artificial neuron</td>
    </tr>
    <tr>
      <td>Signal</td>
      <td>Electrical and chemical</td>
      <td>Numeric (real values)</td>
    </tr>
    <tr>
      <td>Learning</td>
      <td>Hebbian-based, experience-driven</td>
      <td>Error-correction-based (e.g. BP)</td>
    </tr>
    <tr>
      <td>Structure</td>
      <td>Irregular, complex</td>
      <td>Structured layers (input, hidden, output)</td>
    </tr>
  </tbody>
</table>
<br><br><br>
<h2><mark>2. ANN Architecture</mark></h2>

<h3>Layers of ANN:</h3>
<ul>
  <li><strong>Input Layer:</strong> Raw data input (e.g. pixels of an image).</li>
  <li><strong>Hidden Layer(s):</strong> Processes data, extracts patterns.</li>
  <li><strong>Output Layer:</strong> Provides prediction/output.</li>
</ul>

<h3>Data Flow:</h3>
<ol>
  <li>Input ‚Üí Linear combination (weights & bias)</li>
  <li>Activation function applied</li>
  <li>Output passed to next layer</li>
</ol>

<h3>Example: Image Classifier</h3>
<ul>
  <li><strong>Input:</strong> 784 pixels (28x28 image)</li>
  <li><strong>Hidden Layer 1:</strong> 128 neurons</li>
  <li><strong>Hidden Layer 2:</strong> 64 neurons</li>
  <li><strong>Output Layer:</strong> 10 neurons (digits 0‚Äì9)</li>
</ul>
<p>Each neuron in a layer is connected to every neuron in the next layer (fully connected).</p>
<br><br><br>

<h2><mark>3. Basic Building Block of an Artificial Neuron</mark></h2>

<h3>Mathematical Model:</h3>
<p>
  <strong>y = f(z) = f(‚àë<sub>i=1</sub><sup>n</sup> w<sub>i</sub>x<sub>i</sub> + b)</strong>
</p>
<ul>
  <li><strong>x<sub>i</sub>:</strong> Inputs</li>
  <li><strong>w<sub>i</sub>:</strong> Weights</li>
  <li><strong>b:</strong> Bias</li>
  <li><strong>f:</strong> Activation function (non-linear)</li>
  <li><strong>y:</strong> Output</li>
</ul>

<h3> Intuitive Example:</h3>
<p>Suppose we want to determine if a student passes based on:</p>
<ul>
  <li>Attendance (x‚ÇÅ = 80%)</li>
  <li>Homework score (x‚ÇÇ = 70%)</li>
</ul>
<p>Weights:</p>
<ul>
  <li>w‚ÇÅ = 0.3</li>
  <li>w‚ÇÇ = 0.6</li>
  <li>b = 5</li>
</ul>
<p><strong>Calculation:</strong></p>
<p>z = 0.3 √ó 80 + 0.6 √ó 70 + 5 = 24 + 42 + 5 = 71</p>
<p>If we apply a threshold of 50 (step function), output = 1 (pass)</p>

<br>

<h2><mark>4. Activation Functions Comparison</mark></h2>
<table border="1" cellpadding="8" cellspacing="0">
  <thead>
    <tr>
      <th>Function</th>
      <th>Formula</th>
      <th>Shape & Use</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Step</strong></td>
      <td>1 if x &gt; Œ∏ else 0</td>
      <td>Binary classification</td>
    </tr>
    <tr>
      <td><strong>Sigmoid</strong></td>
      <td>1 / (1 + e<sup>-x</sup>)</td>
      <td>Smooth, output between (0, 1)</td>
    </tr>
    <tr>
      <td><strong>Tanh</strong></td>
      <td>(e<sup>x</sup> - e<sup>-x</sup>) / (e<sup>x</sup> + e<sup>-x</sup>)</td>
      <td>Output between (-1, 1)</td>
    </tr>
    <tr>
      <td><strong>ReLU</strong></td>
      <td>max(0, x)</td>
      <td>Used in deep learning, efficient</td>
    </tr>
    <tr>
      <td><strong>Leaky ReLU</strong></td>
      <td>max(0.01x, x)</td>
      <td>Solves dead ReLU problem</td>
    </tr>
  </tbody>
</table>

<h3>Example:</h3>
<p>If the neuron receives z = 2:</p>
<ul>
  <li><strong>Sigmoid:</strong> ~0.88</li>
  <li><strong>ReLU:</strong> 2</li>
  <li><strong>Tanh:</strong> ~0.96</li>
</ul>

<br><br>

<h2><mark>5. Early ANN Architectures (Basics Only)</mark></h2>

<h3>McCulloch & Pitts Model (1943)</h3>
<p>Simplest binary neuron.</p>
<ul>
  <li><strong>Output:</strong> 1 if weighted sum ‚â• threshold.</li>
  <li>No learning ‚Äì fixed weights.</li>
</ul>
<p><strong> Example: AND gate</strong></p>
<ul>
  <li>Inputs: x‚ÇÅ = 1, x‚ÇÇ = 1</li>
  <li>Weights = [1, 1], Threshold = 2</li>
  <li>z = 1√ó1 + 1√ó1 = 2 ‚áí Output = 1</li>
</ul>

<h3>Perceptron (1958)</h3>
<p>Invented by Frank Rosenblatt</p>
<ul>
  <li>Single-layer neural network</li>
  <li>Learns using Perceptron Learning Rule:</li>
</ul>
<p><strong>w<sub>i</sub> = w<sub>i</sub> + Œîw<sub>i</sub>, where Œîw<sub>i</sub> = Œ∑(t ‚àí o)x<sub>i</sub></strong></p>
<ul>
  <li><strong>t:</strong> Target output</li>
  <li><strong>o:</strong> Actual output</li>
  <li><strong>Œ∑:</strong> Learning rate</li>
</ul>
<p><strong> Example: Learn the OR gate</strong></p>
<table border="1" cellpadding="6">
  <thead>
    <tr>
      <th>x‚ÇÅ</th><th>x‚ÇÇ</th><th>Output</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>0</td><td>0</td><td>0</td></tr>
    <tr><td>0</td><td>1</td><td>1</td></tr>
    <tr><td>1</td><td>0</td><td>1</td></tr>
    <tr><td>1</td><td>1</td><td>1</td></tr>
  </tbody>
</table>
<p>Weights adjust over iterations to correctly classify all inputs.</p>
<p><strong> Limitation:</strong> Only works for linearly separable data.</p>

<h3>ADALINE (Adaptive Linear Neuron) ‚Äì by Widrow & Hoff</h3>
<ul>
  <li>Similar to perceptron, but output is <strong>continuous</strong>, not binary.</li>
  <li>Uses LMS (Least Mean Squares) error correction:</li>
</ul>
<p><strong>E = (1/2) ‚àë(t ‚àí o)<sup>2</sup></strong></p>
<p><strong>Example:</strong> Used in filtering signals, e.g., noise removal from audio.</p>

<h3>MADALINE (Multiple ADALINE)</h3>
<ul>
  <li>Multi-layer extension of ADALINE.</li>
  <li>First network applied to real-world problems.</li>
  <li>Uses ‚Äúrule-based‚Äù weight updates, not backpropagation.</li>
</ul>
<p><strong>Example:</strong> Used by AT&T for echo cancellation in telephone networks.</p>

<h3> Recap:</h3>
<table border="1" cellpadding="8">
  <thead>
    <tr>
      <th>Model</th>
      <th>Type</th>
      <th>Learning</th>
      <th>Key Feature</th>
      <th>Example Use</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>McCulloch-Pitts</td>
      <td>Binary neuron</td>
      <td>None</td>
      <td>Fixed weights and threshold</td>
      <td>Logic gates (AND, OR)</td>
    </tr>
    <tr>
      <td>Perceptron</td>
      <td>Single-layer</td>
      <td>Perceptron Rule</td>
      <td>Classifies linearly separable data</td>
      <td>Binary classification</td>
    </tr>
    <tr>
      <td>ADALINE</td>
      <td>Single-layer</td>
      <td>LMS Rule</td>
      <td>Continuous output</td>
      <td>Signal processing</td>
    </tr>
    <tr>
      <td>MADALINE</td>
      <td>Multi-layer</td>
      <td>Rule-based</td>
      <td>Real-world applications</td>
      <td>Echo cancellation</td>
    </tr>
  </tbody>
</table>

<br><br>
<br>
<!-- module-4  -->

<mark><strong>Module :- 04</strong></mark><br><br>
<h2>Artificial Neural Networks: Supervised Learning</h2>

<h3><mark>1. Backpropagation Networks</mark></h3>
<p>Backpropagation is a supervised learning algorithm used for training multi-layer neural networks by minimizing the error between predicted and actual outputs. It is an example of gradient descent optimization.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Input Layer:</strong> Receives features (e.g., pixels in an image).</li>
  <li><strong>Hidden Layers:</strong> Perform computations, extract patterns.</li>
  <li><strong>Output Layer:</strong> Produces final prediction (e.g., image classification).</li>
</ul>

<p><strong>Backpropagation Learning:</strong></p>
<ul>
  <li><strong>Forward pass:</strong> Input data is passed through the network to generate output.</li>
  <li><strong>Calculate error:</strong> Compare predicted output with actual target (using loss function).</li>
  <li><strong>Backward pass:</strong> The error is propagated backward to adjust weights.</li>
  <li><strong>Weight update:</strong> Using gradient descent, weights are updated to minimize the error.</li>
</ul>

<p><strong>Mathematical Update Rule:</strong></p>
<p><strong>Œîw = ‚àíŒ∑ ‚àÇE / ‚àÇw</strong></p>
<ul>
  <li><strong>Œ∑:</strong> Learning rate.</li>
  <li><strong>E:</strong> Error (difference between actual and predicted values).</li>
  <li><strong>w:</strong> Weight of the network.</li>
</ul>

<p><strong>Example:</strong> For an image classification task (classifying images into categories), backpropagation updates the weights of neurons in hidden layers to reduce the classification error.</p>

<h3><mark>2. Multilayer Perceptron (MLP)</mark></h3>
<p>A Multilayer Perceptron is a type of neural network that consists of one input layer, one or more hidden layers, and one output layer. It uses backpropagation for training.</p>

<p><strong>Structure:</strong></p>
<ul>
  <li><strong>Input Layer:</strong> Size equal to the number of input features.</li>
  <li><strong>Hidden Layer(s):</strong> Contains neurons connected to each other, responsible for complex computations.</li>
  <li><strong>Output Layer:</strong> Determines the prediction (classification or regression output).</li>
</ul>

<p><strong>Example:</strong> In a binary classification task (spam detection):</p>
<ul>
  <li><strong>Input:</strong> Email features like word frequencies.</li>
  <li><strong>Output:</strong> 1 (spam) or 0 (not spam).</li>
</ul>

<h3><mark>3. Accelerated Learning in Multilayer Perceptron</mark></h3>
<p>To speed up learning, several methods can be used in MLP:</p>
<ul>
  <li><strong>Momentum:</strong> Helps avoid local minima by adding a fraction of the previous weight update to the current one.</li>
  <li><strong>Adaptive Learning Rate:</strong> Algorithms like Adam adjust the learning rate during training for more efficient convergence.</li>
</ul>

<p><strong>Example:</strong> Using Adam optimizer, the network can adjust its learning rate dynamically, leading to faster convergence and more accurate predictions.</p>

<h3><mark>4. Hopfield Network</mark></h3>
<p>A Hopfield Network is a recurrent neural network used for associative memory, meaning it can recall a pattern from partial information.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Fully connected neurons with symmetric weights (no feedback loops).</li>
  <li>Bipolar output: Each neuron outputs either 1 or -1.</li>
  <li><strong>Energy Function:</strong> The network seeks to minimize its energy, reaching a stable state (attractor).</li>
</ul>

<p><strong>Example:</strong> If the network is trained to store binary patterns (e.g., 1010), it can recall the full pattern even from a noisy or incomplete input.</p>

<h3><mark>5. Bidirectional Associative Memory (BAM)</mark></h3>
<p>A BAM is a type of recurrent network consisting of two layers of neurons (input and output) with bidirectional connections.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Two layers: One layer for inputs and another for outputs.</li>
  <li>Symmetric connections: Weights are bidirectional, meaning either layer can serve as input or output.</li>
</ul>

<p><strong>Example:</strong> In a BAM, if you input a partial pattern (e.g., first half of a word), the network can recall the other half.</p>

<h3><mark>6. Radial Basis Function (RBF) Neural Network</mark></h3>
<p>The RBF Neural Network is a type of feedforward network used for function approximation and classification.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Input layer:</strong> Receives input features.</li>
  <li><strong>Hidden layer:</strong> Consists of RBF neurons that use radial functions (like Gaussian) to compute activations.</li>
  <li><strong>Output layer:</strong> Produces output based on the weighted sum of activations.</li>
</ul>

<p><strong>Example:</strong> For a classification task (e.g., recognizing objects), the RBF network can learn from labeled examples and classify new instances based on proximity to the center of the Gaussian functions.</p>

<h2>Artificial Neural Networks: Unsupervised Learning</h2>

<h3><mark>1. Hebbian Learning</mark></h3>
<p>Hebbian Learning is based on the principle "neurons that fire together, wire together." This means if two neurons are activated together, the strength of their connection (synapse) increases.</p>

<p><strong>Mathematical Rule:</strong></p>
<p><strong>Œîw = Œ∑xy</strong></p>
<ul>
  <li><strong>x and y:</strong> Inputs to the connected neurons.</li>
  <li><strong>Œ∑:</strong> Learning rate.</li>
  <li><strong>w:</strong> Weight.</li>
</ul>

<p><strong>Example:</strong> If two neurons are activated simultaneously (say, when a dog and a bone are seen together), their connection will strengthen, making it easier for the network to recognize a dog-bone pair in future images.</p>

<h3><mark>2. Generalized Hebbian Learning Algorithm (GHLA)</mark></h3>
<p>The Generalized Hebbian Learning Algorithm is an extension of Hebbian learning used to optimize neuron connections in a multi-layer network.</p>

<p><strong>Application:</strong></p>
<ul>
  <li>Used for principal component analysis (PCA) and unsupervised learning tasks.</li>
  <li>Helps in identifying correlations in data without needing labeled examples.</li>
</ul>

<h3><mark>3. Competitive Learning</mark></h3>
<p>Competitive Learning is a type of unsupervised learning where neurons "compete" to become active in response to an input. The winner neuron (with the most similar weights to the input) gets updated.</p>

<p><strong>Example:</strong> In K-means clustering, a set of data points is divided into groups. Each neuron represents a group (cluster) and is updated to better match the data points assigned to it.</p>

<h3><mark>4. Self-Organizing Computational Maps (SOM) ‚Äì Kohonen Network</mark></h3>
<p>The Self-Organizing Map (SOM), developed by Teuvo Kohonen, is an unsupervised learning algorithm that maps high-dimensional data to a lower-dimensional grid (usually 2D).</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li>A grid of neurons is trained to represent clusters of input data.</li>
  <li>The neurons are connected and organized in a manner that similar inputs activate nearby neurons.</li>
</ul>

<p><strong>Example:</strong> Input: Data points from various customer preferences. SOM organizes these data points on a 2D grid where similar customers are grouped together, useful for market segmentation.</p>

<h2>Summary of Key Concepts</h2>
<table border="1" cellpadding="8">
  <thead>
    <tr>
      <th>Model</th>
      <th>Type</th>
      <th>Learning Type</th>
      <th>Key Feature</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Backpropagation</td>
      <td>Supervised</td>
      <td>Supervised</td>
      <td>Updates weights to minimize error</td>
      <td>Image classification (MNIST)</td>
    </tr>
    <tr>
      <td>Multilayer Perceptron</td>
      <td>Feedforward</td>
      <td>Supervised</td>
      <td>Deep neural network for classification</td>
      <td>Spam detection</td>
    </tr>
    <tr>
      <td>Hopfield Network</td>
      <td>Recurrent</td>
      <td>Unsupervised</td>
      <td>Used for associative memory</td>
      <td>Pattern retrieval</td>
    </tr>
    <tr>
      <td>Bidirectional Associative Memory (BAM)</td>
      <td>Recurrent</td>
      <td>Unsupervised</td>
      <td>Recalls full pattern from partial input</td>
      <td>Image reconstruction</td>
    </tr>
    <tr>
      <td>RBF Network</td>
      <td>Feedforward</td>

</tr>
  </tbody>
  </table>
      <br><br>
      <!-- module-5  -->
      <h1>Genetic Algorithms (GAs)</h1>

      <h2><mark>üîπ 1. Basic Concepts</mark></h2>
      <ul>
        <li><strong>Population:</strong> A group of candidate solutions.</li>
        <li><strong>Chromosomes:</strong> A single candidate solution.</li>
        <li><strong>Genes:</strong> Parts of a chromosome (variables).</li>
        <li><strong>Fitness:</strong> How well a solution solves the problem.</li>
        <li><strong>Generations:</strong> Iterations in the GA process.</li>
      </ul>
    
      <h2><mark>üîπ 2. Encoding (Chromosome Representation)</mark></h2>
      <p>Solutions are encoded for the GA to process.</p>
      <p><strong>Example: Binary Encoding</strong></p>
      <ul>
        <li>Function: f(x) = x¬≤, where 0 ‚â§ x ‚â§ 31</li>
        <li>x = 6 ‚Üí Binary = 00110</li>
        <li>f(x) = 36</li>
      </ul>
      <p>Other encodings include:</p>
      <ul>
        <li>Real-valued encoding (for optimization problems)</li>
        <li>Tree encoding (used in Genetic Programming)</li>
      </ul>
    
      <h2><mark>üîπ 3. Fitness Function</mark></h2>
      <p>Evaluates how good a solution is.</p>
      <p><strong>Example:</strong> f(x) = x¬≤, x = 6 (00110) ‚Üí Fitness = 36</p>
    
      <h2><mark>üîπ 4. Selection Methods (Reproduction)</mark></h2>
      <p>Select the best individuals (parents) for crossover.</p>
      <table border="1">
        <tr><th>Method</th><th>Description</th></tr>
        <tr><td>Roulette Wheel</td><td>Selection probability ‚àù fitness</td></tr>
        <tr><td>Tournament</td><td>Random subset; select best</td></tr>
        <tr><td>Rank Selection</td><td>Based on rank, not actual fitness</td></tr>
        <tr><td>Steady State</td><td>Only a few individuals replaced</td></tr>
      </table>
      <p><strong>Example (Roulette Wheel):</strong> Fitness values = [10, 30, 60] ‚Üí Probabilities = [10%, 30%, 60%]</p>
    
      <h2><mark>üîπ 5. Genetic Operators</mark></h2>
    
      <h3><mark>üî∏ a) Crossover (Recombination)</mark></h3>
      <p>Combines two parents to create offspring.</p>
      <ul>
        <li>Parent 1: 11001</li>
        <li>Parent 2: 10110</li>
        <li>Crossover at point 3 ‚Üí Offspring 1: 11010, Offspring 2: 10101</li>
      </ul>
    
      <h3><mark>üî∏ b) Mutation</mark></h3>
      <p>Randomly changes a gene to maintain diversity.</p>
      <ul>
        <li>Before: 11001</li>
        <li>Mutation at 3rd bit ‚Üí After: 11101</li>
      </ul>
      <h2><mark>üîπ 6. Convergence of GA</mark></h2>
      <ul>
        <li>GA converges when the population becomes stable (no new better solutions).</li>
        <li>Best solution remains the same for many generations.</li>
        <li><strong>Premature convergence:</strong> Suboptimal solutions due to lack of diversity.</li>
      </ul>
    
      <h2><mark>üîπ 7. Applications of GA</mark></h2>
      <h3><mark>Example 1: Traveling Salesman Problem (TSP)</mark></h3>
      <ul>
        <li><strong>Encoding:</strong> Permutation of city indices</li>
        <li><strong>Fitness:</strong> Total distance</li>
        <li><strong>Crossover:</strong> Partially mapped crossover</li>
        <li><strong>Mutation:</strong> Swap cities</li>
      </ul>
    
      <h3><mark> Example 2: Function Optimization</mark></h3>
      <ul>
        <li>Maximize f(x) = x ‚ãÖ sin(x)</li>
        <li>Use real-valued encoding</li>
        <li>Evaluate f(x) for each individual</li>
      </ul>
    
      <h2><mark>üîπ 8. Genetic Programming (GP)</mark></h2>
      <p>Genetic Programming is a GA-based method to evolve programs or expressions.</p>
      <ul>
        <li><strong>Operators:</strong> +, -, *, /</li>
        <li><strong>Terminals:</strong> Constants, variables</li>
      </ul>
      <p><strong>Example:</strong> Evolving the function f(x) = x¬≤ + 2x + 1</p>
      <pre>
            +
           / \
          *   +
         / \  / \
        x  x 2  1
      </pre>
      <p><strong>Mutation:</strong> Change + to -</p>
      <p><strong>Crossover:</strong> Swap subtrees between programs</p>
    
      <h2><mark> Computational Swarm Intelligence (CSI)</mark></h2>
    
      <h3><mark>üîπ 1. Particle Swarm Optimization (PSO)</mark></h3>
      <p>Inspired by bird flocking or fish schooling.</p>
      <ul>
        <li>Each particle has a position (solution) and velocity (direction).</li>
        <li>Updates based on:</li>
        <ul>
          <li>Its own best position (pBest)</li>
          <li>Global best position (gBest)</li>
        </ul>
      </ul>
      <p><strong>Update Equations:</strong></p>
      <pre>
    v·µ¢ = v·µ¢ + c‚ÇÅ¬∑r‚ÇÅ¬∑(pBest ‚àí x·µ¢) + c‚ÇÇ¬∑r‚ÇÇ¬∑(gBest ‚àí x·µ¢)
    x·µ¢ = x·µ¢ + v·µ¢
      </pre>
      <p><strong>Example:</strong> Minimize f(x) = (x‚àí3)¬≤</p>
      <ul>
        <li>Start: x‚ÇÅ = 5, x‚ÇÇ = 1 ‚Üí f(5)=4, f(1)=4</li>
        <li>Update velocities and move toward x=3</li>
        <li>Particles converge toward optimal value</li>
      </ul>
    
      <h3><mark>üîπ 2. Ant Colony Optimization (ACO)</mark></h3>
      <p>Inspired by ants finding the shortest path using pheromone trails.</p>
      <ul>
        <li>Ants choose paths based on pheromone concentration.</li>
        <li>Better paths ‚Üí more pheromones.</li>
        <li>More pheromones attract more ants.</li>
        <li>Pheromones evaporate over time to prevent stagnation.</li>
      </ul>
      <p><strong>Example:</strong> Solving TSP (Cities = Nodes, Roads = Paths)</p>
      <ul>
        <li>Pheromone trails guide ants to shortest loop</li>
        <li>Most efficient path dominates over iterations</li>
      </ul>
    
      <h2><mark> Summary Table</mark></h2>
      <table border="1">
        <tr><th>Topic</th><th>Concept</th><th>Example</th></tr>
        <tr><td>Genetic Algorithm</td><td>Natural selection</td><td>Optimize x¬≤, TSP</td></tr>
        <tr><td>Fitness Function</td><td>Measures solution quality</td><td>f(x)=x¬≤</td></tr>
        <tr><td>Crossover</td><td>Combines parent chromosomes</td><td>11001 + 10110 ‚Üí 11010</td></tr>
        <tr><td>Mutation</td><td>Bit flip for diversity</td><td>11001 ‚Üí 11101</td></tr>
        <tr><td>Genetic Programming</td><td>Evolves trees/programs</td><td>x¬≤ + 2x + 1</td></tr>
        <tr><td>PSO</td><td>Particles + Velocity</td><td>Minimize (x‚àí3)¬≤</td></tr>
        <tr><td>ACO</td><td>Ants + Pheromones</td><td>Shortest route in TSP</td></tr>
      </table>
    
        </body>
</html>